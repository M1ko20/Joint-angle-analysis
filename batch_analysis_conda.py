#!/usr/bin/env python3
"""
Batch anal√Ωza CONDA (MMPose) model≈Ø na v≈°ech vide√≠ch
Spou≈°t√≠ pouze conda modely (HRNet, RTMPose, RTMPose3D) p≈ôes subprocess
Pro venv modely pou≈æij batch_analysis_venv.py
"""

import os
import sys
import json
import cv2
import numpy as np
from pathlib import Path
from datetime import datetime
import subprocess
import tempfile


class BatchAnalyzerConda:
    """Batch anal√Ωza CONDA (MMPose) model≈Ø na v≈°ech vide√≠ch"""
    
    def __init__(self, videos_root="video", output_root="output", confidence_threshold=0.5, 
                 conda_env="openmmlab", use_gpu=True):
        self.videos_root = Path(videos_root)
        self.output_root = Path(output_root)
        self.confidence_threshold = confidence_threshold
        self.conda_env = conda_env
        self.use_gpu = use_gpu
        self.temp_videos = []  # Pro sledov√°n√≠ doƒçasn√Ωch rotovan√Ωch vide√≠
        
        # Seznam conda model≈Ø
        self.conda_models = self._get_conda_models()
        
        print(f"\n{'='*80}")
        print(f"üéØ BATCH POSE ANALYSIS - CONDA (MMPose) MODELY")
        print(f"{'='*80}")
        print(f"üìÅ Videa: {self.videos_root}")
        print(f"üìÅ Output: {self.output_root}")
        print(f"üéöÔ∏è Confidence: {self.confidence_threshold}")
        print(f"üêç Conda env: {self.conda_env}")
        print(f"üéÆ GPU: {'‚úÖ Zapnuto' if self.use_gpu else '‚ùå Vypnuto (CPU)'}")
        print(f"üîß Dostupn√© modely: {len(self.conda_models)}")
        for model in self.conda_models:
            print(f"   ‚Ä¢ {model}")
        print(f"{'='*80}\n")
    
    def _get_conda_models(self):
        """Seznam conda model≈Ø (MMPose)"""
        # Kontrola conda prost≈ôed√≠
        try:
            result = subprocess.run(
                ['conda', 'env', 'list'],
                capture_output=True,
                text=True,
                timeout=5
            )
            if self.conda_env in result.stdout:
                return ["hrnet", "rtmpose", "rtmpose3d"]
            else:
                print(f"‚ö†Ô∏è Conda prost≈ôed√≠ '{self.conda_env}' nenalezeno!")
                return []
        except Exception as e:
            print(f"‚ö†Ô∏è Nelze zkontrolovat conda prost≈ôed√≠: {e}")
            return []
    
    def _detect_video_rotation(self, video_path):
        """
        Detekuje pot≈ôebnou rotaci videa pomoc√≠ metadat a anal√Ωzy rozmƒõr≈Ø
        
        Returns:
            int: Rotace ve stupn√≠ch (90, 180, 270) nebo None
        """
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            return None
        
        # Zkus z√≠skat rotation metadata
        try:
            rotation = cap.get(cv2.CAP_PROP_ORIENTATION_META)
            if rotation in [90, 180, 270]:
                cap.release()
                print(f"      üìê Detekov√°na rotace z metadat: {rotation}¬∞")
                return int(rotation)
        except:
            pass
        
        # Anal√Ωza rozmƒõr≈Ø videa
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        aspect_ratio = width / height
        
        cap.release()
        
        # Heuristika: video s lidmi by mƒõlo b√Ωt sp√≠≈°e na ≈°√≠≈ôku nebo ƒçtvercov√©
        if aspect_ratio < 0.8:
            # Video je v√Ωraznƒõ na v√Ω≈°ku ‚Üí pravdƒõpodobnƒõ pot≈ôebuje rotaci
            print(f"      üìê Video je na v√Ω≈°ku (aspect ratio: {aspect_ratio:.2f}), aplikuji rotaci 90¬∞")
            return 90
        elif aspect_ratio > 1.5:
            # Video je velmi ≈°irok√©, mo≈æn√° je otoƒçen√© o 90¬∞ a mƒõlo by b√Ωt na v√Ω≈°ku
            print(f"      üìê Video je velmi ≈°irok√© (aspect ratio: {aspect_ratio:.2f}), mo≈æn√° rotace 270¬∞")
            return 270
        
        print(f"      üìê Video se zd√° spr√°vnƒõ orientovan√© (aspect ratio: {aspect_ratio:.2f})")
        return None
    
    def _rotate_video(self, video_path, rotation_degrees):
        """
        Rotuje video a vrac√≠ cestu k doƒçasn√©mu souboru
        
        Args:
            video_path: Cesta k p≈Øvodn√≠mu videu
            rotation_degrees: Stupnƒõ rotace (90, 180, 270)
        
        Returns:
            Path k rotovan√©mu videu
        """
        if rotation_degrees is None or rotation_degrees == 0:
            return video_path
        
        print(f"      üîÑ Rotuji video o {rotation_degrees}¬∞...")
        
        # Mapov√°n√≠ rotace
        rotation_map = {
            90: cv2.ROTATE_90_CLOCKWISE,
            180: cv2.ROTATE_180,
            270: cv2.ROTATE_90_COUNTERCLOCKWISE
        }
        
        if rotation_degrees not in rotation_map:
            print(f"      ‚ö†Ô∏è Neplatn√° rotace: {rotation_degrees}¬∞")
            return video_path
        
        rotation_code = rotation_map[rotation_degrees]
        
        # Vytvo≈ô doƒçasn√Ω soubor
        temp_dir = Path(tempfile.gettempdir()) / "batch_analysis_rotated"
        temp_dir.mkdir(exist_ok=True)
        
        temp_video = temp_dir / f"{video_path.stem}_rotated_{rotation_degrees}.mp4"
        
        # Naƒçti video
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            print(f"      ‚ùå Nelze otev≈ô√≠t video")
            return video_path
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        # Zjisti nov√© rozmƒõry
        ret, first_frame = cap.read()
        if not ret:
            cap.release()
            return video_path
        
        rotated_first = cv2.rotate(first_frame, rotation_code)
        new_height, new_width = rotated_first.shape[:2]
        
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        
        # Vytvo≈ô v√Ωstup
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(temp_video), fourcc, fps, (new_width, new_height))
        
        frame_count = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            rotated_frame = cv2.rotate(frame, rotation_code)
            out.write(rotated_frame)
            frame_count += 1
        
        cap.release()
        out.release()
        
        print(f"      ‚úÖ Video rotov√°no ({frame_count} fram≈Ø) ‚Üí {temp_video.name}")
        
        # Zapamatuj pro cleanup
        self.temp_videos.append(temp_video)
        
        return temp_video
    
    def get_videos(self):
        """Najde v≈°echna videa v side/front slo≈æk√°ch"""
        videos = []
        for view in ['side', 'front']:
            view_path = self.videos_root / view
            if view_path.exists():
                for video_file in view_path.glob("*.mp4"):
                    # N√°zev slo≈æky bez p≈ô√≠pony, oprava p≈ôeklep≈Ø
                    video_name = video_file.stem
                    if video_name == "minustwetny":
                        video_name = "minustwenty"
                    
                    videos.append({
                        'path': video_file,
                        'view': view,
                        'name': video_name
                    })
        return videos
    
    def run_all(self):
        """Spust√≠ anal√Ωzu v≈°ech conda model≈Ø na v≈°ech vide√≠ch"""
        if not self.conda_models:
            print("‚ùå ≈Ω√°dn√© conda modely dostupn√©!")
            return
        
        videos = self.get_videos()
        
        if not videos:
            print("‚ùå ≈Ω√°dn√° videa nenalezena!")
            return
        
        print(f"üìπ Nalezeno {len(videos)} vide√≠:")
        for v in videos:
            print(f"   ‚Ä¢ {v['view']}/{v['name']}.mp4")
        print()
        
        # Sekvenƒçn√≠ zpracov√°n√≠
        total = len(self.conda_models) * len(videos)
        current = 0
        
        results_summary = []
        
        for model in self.conda_models:
            print(f"\n{'='*80}")
            print(f"üîß MODEL: {model.upper()}")
            print(f"{'='*80}\n")
            
            for video in videos:
                current += 1
                print(f"[{current}/{total}] {model} ‚Üí {video['view']}/{video['name']}")
                
                try:
                    result = self.analyze_video(model, video)
                    results_summary.append(result)
                    
                    if result['success']:
                        print(f"   ‚úÖ √öspƒõch ({result['time']:.1f}s)")
                    else:
                        print(f"   ‚ùå Selh√°n√≠: {result['error']}")
                
                except Exception as e:
                    print(f"   ‚ùå Kritick√° chyba: {e}")
                    results_summary.append({
                        'model': model,
                        'video': video['name'],
                        'view': video['view'],
                        'success': False,
                        'error': str(e),
                        'time': 0
                    })
        
        # Ulo≈æen√≠ souhrnu
        self._save_summary(results_summary)
        self._print_summary(results_summary)
        
        # Cleanup doƒçasn√Ωch vide√≠
        self._cleanup_temp_videos()
    
    def _cleanup_temp_videos(self):
        """Sma≈æe doƒçasn√° rotovan√° videa"""
        if not self.temp_videos:
            return
        
        print(f"\nüßπ ƒåi≈°tƒõn√≠ doƒçasn√Ωch vide√≠...")
        for temp_video in self.temp_videos:
            try:
                if temp_video.exists():
                    temp_video.unlink()
                    print(f"   ‚Ä¢ Smaz√°no: {temp_video.name}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Nelze smazat {temp_video.name}: {e}")
        
        # Pokus o smaz√°n√≠ slo≈æky
        try:
            temp_dir = Path(tempfile.gettempdir()) / "batch_analysis_rotated"
            if temp_dir.exists() and not list(temp_dir.iterdir()):
                temp_dir.rmdir()
                print(f"   ‚Ä¢ Smaz√°na slo≈æka: {temp_dir}")
        except:
            pass
    
    def analyze_video(self, model, video_info):
        """Analyzuje jedno video jedn√≠m modelem"""
        import time
        start_time = time.time()
        
        video_path = video_info['path']
        view = video_info['view']
        video_name = video_info['name']
        
        # Detekce a aplikace rotace videa
        rotation = self._detect_video_rotation(video_path)
        if rotation:
            video_path = self._rotate_video(video_path, rotation)
        
        # V√Ωstupn√≠ slo≈æka
        output_dir = self.output_root / model / view / video_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            success = self._analyze_conda(model, video_path, output_dir)
            
            return {
                'model': model,
                'video': video_name,
                'view': view,
                'success': success,
                'error': None if success else "Unknown error",
                'time': time.time() - start_time,
                'output_dir': str(output_dir)
            }
        
        except Exception as e:
            return {
                'model': model,
                'video': video_name,
                'view': view,
                'success': False,
                'error': str(e),
                'time': time.time() - start_time,
                'output_dir': str(output_dir)
            }
    
    def _analyze_conda(self, model, video_path, output_dir):
        """Anal√Ωza pomoc√≠ conda modelu (subprocess)"""
        # Vytvo≈ô doƒçasn√Ω skript
        temp_script = self._create_conda_script(model, video_path, output_dir)
        
        try:
            # Spus≈• v conda
            cmd = [
                'conda', 'run', '-n', self.conda_env, '--no-capture-output',
                'python', str(temp_script)
            ]
            
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = process.communicate(timeout=600)  # 10 min timeout
            
            # Print v√Ωstup z conda procesu
            if stdout:
                print(f"      {stdout.strip()}")
            
            if process.returncode != 0:
                raise RuntimeError(f"Conda process failed: {stderr[:500]}")
            
            return True
        
        finally:
            # Sma≈æ doƒçasn√Ω skript
            if temp_script.exists():
                temp_script.unlink()
    
    def _create_conda_script(self, model, video_path, output_dir):
        """Vytvo≈ô√≠ doƒçasn√Ω Python skript pro conda - p≈ô√≠mo importuje MMPose"""
        analysis_dir = Path(__file__).parent.absolute()
        
        # Vytvo≈ô spr√°vn√Ω skript podle modelu
        if model in ['hrnet', 'rtmpose']:
            script_content = self._generate_mmpose_script(model, video_path, output_dir, analysis_dir)
        elif model == 'rtmpose3d':
            script_content = self._generate_rtmpose3d_script(video_path, output_dir, analysis_dir)
        else:
            raise ValueError(f"Nezn√°m√Ω conda model: {model}")
        
        temp_file = output_dir / "_temp_conda_script.py"
        with open(temp_file, 'w') as f:
            f.write(script_content)
        
        return temp_file
    
    def _generate_mmpose_script(self, model, video_path, output_dir, analysis_dir):
        """Generuje skript pro HRNet/RTMPose - p≈ô√≠mo importuje MMPose"""
        # Cesty k config a checkpoint soubor≈Øm
        if model == 'hrnet':
            config_file = analysis_dir / 'td-hm_hrnet-w48_8xb32-210e_coco-256x192.py'
            checkpoint_file = analysis_dir / 'td-hm_hrnet-w48_8xb32-210e_coco-256x192-0e67c616_20220913.pth'
        else:  # rtmpose
            config_file = analysis_dir / 'RTMPose' / 'rtmpose-l_8xb256-420e_coco-384x288.py'
            checkpoint_file = analysis_dir / 'RTMPose' / 'rtmpose-l_simcc-aic-coco_pt-aic-coco_420e-384x288-97d6cb0f_20230228.pth'
        
        device = 'cuda' if self.use_gpu else 'cpu'
        
        return f"""import cv2
import json
import numpy as np
from pathlib import Path

# Import MMPose p≈ô√≠mo
from mmpose.apis import inference_topdown, init_model
from mmpose.utils import register_all_modules
register_all_modules()

# Inicializace modelu
print("üîß Inicializuji {model} na {device.upper()}...")
model = init_model(r'{config_file}', r'{checkpoint_file}', device='{device}')
print("‚úÖ Model naƒçten")

# Zpracov√°n√≠ videa
cap = cv2.VideoCapture(r'{video_path}')
fps = cap.get(cv2.CAP_PROP_FPS)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

output_dir = Path(r'{output_dir}')
frames_dir = output_dir / "frames"
frames_dir.mkdir(exist_ok=True)

output_video = output_dir / "analyzed_video.mp4"
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(str(output_video), fourcc, fps, (width, height))

# COCO skeleton
connections = [
    (0, 1), (0, 2), (1, 3), (2, 4),
    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),
    (5, 11), (6, 12), (11, 12),
    (11, 13), (13, 15), (12, 14), (14, 16)
]

keypoints_data = []
frame_idx = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    
    # Detekce
    results = inference_topdown(model, frame)
    
    if results and len(results) > 0:
        result = results[0]
        data = result.pred_instances.to_dict()
        keypoints = data.get('keypoints', [])
        scores = data.get('keypoint_scores', [])
        
        if len(keypoints) > 0 and len(scores) > 0:
            kps = keypoints[0]
            scr = scores[0]
            
            # Vykreslen√≠
            for i, (x, y) in enumerate(kps):
                if scr[i] > {self.confidence_threshold}:
                    cv2.circle(frame, (int(x), int(y)), 4, (0, 255, 0), -1)
            
            for start_idx, end_idx in connections:
                if scr[start_idx] > {self.confidence_threshold} and scr[end_idx] > {self.confidence_threshold}:
                    pt1 = (int(kps[start_idx][0]), int(kps[start_idx][1]))
                    pt2 = (int(kps[end_idx][0]), int(kps[end_idx][1]))
                    cv2.line(frame, pt1, pt2, (255, 0, 0), 2)
            
            # Ulo≈æen√≠
            keypoints_data.append({{'frame': frame_idx, 'keypoints': kps.tolist(), 'scores': scr.tolist()}})
    
    # Z√°pis
    cv2.imwrite(str(frames_dir / f"{{frame_idx:05d}}.jpg"), frame)
    out.write(frame)
    frame_idx += 1
    
    # Progress
    if frame_idx % 30 == 0:
        progress = (frame_idx / total_frames) * 100
        print(f"Zpracov√°no: {{progress:.1f}}% ({{frame_idx}}/{{total_frames}})")

cap.release()
out.release()

# Ulo≈æen√≠ dat
with open(output_dir / "data.json", 'w') as f:
    json.dump(keypoints_data, f, indent=2)

print("‚úÖ Hotovo")
"""
    
    def _generate_rtmpose3d_script(self, video_path, output_dir, analysis_dir):
        """Generuje skript pro RTMPose3D - p≈ô√≠mo importuje MMPose/MMDet"""
        mmpose_dir = analysis_dir / "mmpose"
        det_config = mmpose_dir / "projects/rtmpose3d/demo/rtmdet_m_640-8xb32_coco-person.py"
        det_checkpoint = "https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmdet_m_8xb32-100e_coco-obj365-person-235e8209.pth"
        pose_config = mmpose_dir / "projects/rtmpose3d/configs/rtmw3d-l_8xb64_cocktail14-384x288.py"
        pose_checkpoint = mmpose_dir / "rtmw3d-l_8xb64_cocktail14-384x288-794dbc78_20240626.pth"
        
        device = 'cuda' if self.use_gpu else 'cpu'
        
        return f"""import cv2
import json
import numpy as np
from pathlib import Path

# Import MMPose a MMDet p≈ô√≠mo
from mmpose.apis import inference_topdown, init_model
from mmpose.utils import register_all_modules
from mmdet.apis import inference_detector, init_detector
register_all_modules()

# Inicializace
print("üîß Inicializuji RTMPose3D na {device.upper()}...")
print("   Loading detector...")
detector = init_detector(r'{det_config}', r'{det_checkpoint}', device='{device}')
print("   Loading pose model...")
pose_model = init_model(r'{pose_config}', r'{pose_checkpoint}', device='{device}')
print("‚úÖ Modely naƒçteny")

# Zpracov√°n√≠ videa
cap = cv2.VideoCapture(r'{video_path}')
fps = cap.get(cv2.CAP_PROP_FPS)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

output_dir = Path(r'{output_dir}')
frames_dir = output_dir / "frames"
frames_dir.mkdir(exist_ok=True)

output_video = output_dir / "analyzed_video.mp4"
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(str(output_video), fourcc, fps, (width, height))

# COCO skeleton
connections = [
    (0, 1), (0, 2), (1, 3), (2, 4),
    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),
    (5, 11), (6, 12), (11, 12),
    (11, 13), (13, 15), (12, 14), (14, 16)
]

keypoints_data = []
frame_idx = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    
    # Person detection
    det_result = inference_detector(detector, frame)
    pred_inst = det_result.pred_instances.cpu().numpy()
    
    bboxes = pred_inst.bboxes
    scores = pred_inst.scores
    labels = pred_inst.labels
    
    mask = np.logical_and(labels == 0, scores > {self.confidence_threshold})
    bboxes = bboxes[mask]
    
    if len(bboxes) > 0:
        # Pose estimation
        pose_results = inference_topdown(pose_model, frame, bboxes)
        
        if pose_results and len(pose_results) > 0:
            result = pose_results[0]
            data = result.pred_instances.to_dict()
            keypoints = data.get('keypoints', [])
            kp_scores = data.get('keypoint_scores', [])
            
            if len(keypoints) > 0 and len(kp_scores) > 0:
                kps = keypoints[0]  # (17, 3) - x, y, z
                scr = kp_scores[0]
                
                # Vykreslen√≠ (2D projekce)
                for i, (x, y, z) in enumerate(kps):
                    if scr[i] > {self.confidence_threshold}:
                        cv2.circle(frame, (int(x), int(y)), 4, (0, 255, 0), -1)
                
                for start_idx, end_idx in connections:
                    if scr[start_idx] > {self.confidence_threshold} and scr[end_idx] > {self.confidence_threshold}:
                        pt1 = (int(kps[start_idx][0]), int(kps[start_idx][1]))
                        pt2 = (int(kps[end_idx][0]), int(kps[end_idx][1]))
                        cv2.line(frame, pt1, pt2, (255, 0, 0), 2)
                
                # Ulo≈æen√≠
                keypoints_data.append({{'frame': frame_idx, 'keypoints': kps.tolist(), 'scores': scr.tolist()}})
    
    # Z√°pis
    cv2.imwrite(str(frames_dir / f"{{frame_idx:05d}}.jpg"), frame)
    out.write(frame)
    frame_idx += 1
    
    # Progress
    if frame_idx % 30 == 0:
        progress = (frame_idx / total_frames) * 100
        print(f"Zpracov√°no: {{progress:.1f}}% ({{frame_idx}}/{{total_frames}})")

cap.release()
out.release()

# Ulo≈æen√≠ dat
with open(output_dir / "data.json", 'w') as f:
    json.dump(keypoints_data, f, indent=2)

print("‚úÖ Hotovo")
"""
    
    def _save_summary(self, results):
        """Ulo≈æ√≠ souhrn v≈°ech anal√Ωz"""
        summary_file = self.output_root / "batch_summary_conda.json"
        
        data = {
            'timestamp': datetime.now().isoformat(),
            'confidence_threshold': self.confidence_threshold,
            'environment': f'conda ({self.conda_env})',
            'gpu_enabled': self.use_gpu,
            'total_runs': len(results),
            'successful': sum(1 for r in results if r['success']),
            'failed': sum(1 for r in results if not r['success']),
            'results': results
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Souhrn ulo≈æen: {summary_file}")
    
    def _print_summary(self, results):
        """Vyp√≠≈°e souhrn"""
        print(f"\n{'='*80}")
        print(f"üìä SOUHRN BATCH ANAL√ùZY - CONDA")
        print(f"{'='*80}\n")
        
        successful = [r for r in results if r['success']]
        failed = [r for r in results if not r['success']]
        
        print(f"‚úÖ √öspƒõ≈°n√Ωch: {len(successful)}/{len(results)}")
        print(f"‚ùå Selh√°n√≠: {len(failed)}/{len(results)}")
        
        if successful:
            total_time = sum(r['time'] for r in successful)
            avg_time = total_time / len(successful)
            print(f"‚è±Ô∏è Celkov√Ω ƒças: {total_time:.1f}s")
            print(f"‚è±Ô∏è Pr≈Ømƒõrn√Ω ƒças: {avg_time:.1f}s")
        
        if failed:
            print(f"\nSelhan√° zpracov√°n√≠:")
            for r in failed:
                print(f"  ‚Ä¢ {r['model']} ‚Üí {r['view']}/{r['video']}: {r['error'][:80]}")
        
        print(f"\n{'='*80}")


def main():
    import argparse
    parser = argparse.ArgumentParser(description="Batch anal√Ωza CONDA (MMPose) model≈Ø")
    parser.add_argument("--videos", "-v", type=str, default="video",
                       help="Cesta k slo≈æce s videi (default: video)")
    parser.add_argument("--output", "-o", type=str, default="output",
                       help="V√Ωstupn√≠ slo≈æka (default: output)")
    parser.add_argument("--confidence", "-c", type=float, default=0.5,
                       help="Confidence threshold (default: 0.5)")
    parser.add_argument("--conda-env", type=str, default="openmmlab",
                       help="Conda environment (default: openmmlab)")
    parser.add_argument("--cpu", action="store_true",
                       help="Pou≈æ√≠t CPU m√≠sto GPU")
    parser.add_argument("--model", "-m", type=str,
                       help="Spustit pouze konkr√©tn√≠ model")
    
    args = parser.parse_args()
    
    analyzer = BatchAnalyzerConda(
        videos_root=args.videos,
        output_root=args.output,
        confidence_threshold=args.confidence,
        conda_env=args.conda_env,
        use_gpu=not args.cpu
    )
    
    # Filtrov√°n√≠ podle argument≈Ø
    if args.model:
        analyzer.conda_models = [m for m in analyzer.conda_models if m == args.model]
        if not analyzer.conda_models:
            print(f"‚ùå Model '{args.model}' nen√≠ dostupn√Ω!")
            return 1
    
    analyzer.run_all()
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
